{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69ebdc3-af82-4ecb-88aa-1ee8c61e83f4",
   "metadata": {},
   "source": [
    "# Get Started With NVIDIA Biomedical AI-Q Research Agent Blueprint Using NVIDIA API\n",
    "\n",
    "This notebook helps you get started with the [Biomedical AI-Q Research Agent](https://build.nvidia.com/nvidia/biomedical-aiq-research-agent).\n",
    "\n",
    "\n",
    "## Prerequisites \n",
    "\n",
    "- This blueprint depends on the [NVIDIA RAG Blueprint](https://github.com/NVIDIA-AI-Blueprints/rag). This deployment guide starts by deploying RAG using docker compose, but you should refer to the [RAG Blueprint documentation](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/quickstart.md) for full details. \n",
    "\n",
    "- Docker Compose\n",
    "\n",
    "- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\n",
    "\n",
    "- (Optional) This blueprint supports Tavily web search to supplement data from RAG. A Tavily API key can be supplied to enable this function. \n",
    "\n",
    "- [NVIDIA API Key](https://build.nvidia.com) This notebook uses NVIDIA NIM microservices hosted on build.nvidia.com. To deploy the NIM microservices locally, follow the [getting started deployment guide](https://github.com/NVIDIA-AI-Blueprints/biomedical-aiq-research-agent/blob/main/docs/get-started/get-started-docker-compose.md).\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "This notebook uses NVIDIA NIM microservices hosted on build.nvidia.com for the majority of the services that require GPUs. \n",
    "\n",
    "To run this notebook requires:\n",
    "-  1xL40S or comparable\n",
    "-  72 GB of disk space if deploying RAG ingestion NIMs locally as recommended, or 37 GB of disk space if using all public hosted NIM services\n",
    "-  16 CPUs\n",
    "\n",
    "### NVIDIA NIM Microservices\n",
    "\n",
    "Access NVIDIA NIM microservices including:   \n",
    "- NemoRetriever  \n",
    "  - Page Elements  \n",
    "  - Table Structure  \n",
    "  - Graphic Elements  \n",
    "  - Paddle OCR   \n",
    "- Llama Instruct 3.3 70B  \n",
    "- Llama Nemotron 3.3 Super 49B  \n",
    "- BioNeMo MolMIM\n",
    "- BioNeMo DiffDock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec310ee9-9e67-4c9c-87e8-dfc74305b5f4",
   "metadata": {},
   "source": [
    "## Step 1: Deploy the RAG Blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e70bde-5052-4f32-881a-6d634e2d3091",
   "metadata": {},
   "source": [
    "See the NVIDIA RAG blueprint documentation for full details. This notebook will use docker compose to deploy the RAG blueprint with *hosted NVIDIA NIM microservices*. Start by setting the appropriate environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb8e53-b8e5-4c91-8080-11507edf6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pull images required by the blueprint from NGC, you must first authenticate Docker with nvcr.io.\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# ADD YOUR API KEY\n",
    "NVIDIA_API_KEY = \"nvapi-your-api-key\"\n",
    "os.environ['NVIDIA_API_KEY'] = NVIDIA_API_KEY\n",
    "os.environ['NGC_API_KEY'] = NVIDIA_API_KEY\n",
    "\n",
    "cmd = f\"echo {NVIDIA_API_KEY} | docker login nvcr.io -u '$oauthtoken' --password-stdin\"\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd40603-b3bd-4a2b-a92f-4e5023ac7c37",
   "metadata": {},
   "source": [
    "Next, clone the NVIDIA RAG blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82777f6-df62-4f0d-a27e-70b9e2731287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone the github repository\n",
    "!git clone https://github.com/NVIDIA-AI-Blueprints/rag.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f513df",
   "metadata": {},
   "source": [
    "#### Locally Host vs Public Endpoint for RAG Ingestion NIMs\n",
    "It is recommended in this notebook to deploy the RAG Blueprint ingestion NIMs locally, and keeping the following `deploy_rag_ingestion_locally = True` will do so. The RAG Ingestion NIMs are the only ones that by default will be deployed locally, all other NIMs in the RAG Blueprint and the Biomedical AI-Q Research Agent Developer Blueprint are by default set to the public hosted NVIDIA AI Endpoints in this notebook. However, if you do not want to deploy the RAG ingestion NIMs locally, you can choose to use the public NVIDIA AI Endpoints with your valid NVIDIA_API_KEY, by setting `deploy_rag_ingestion_locally = False`\n",
    "\n",
    "If you're in the Brev launchable, the instance type should be compatible with the default local deployment, which is recommended for the Brev experience. In this case, no need to change anything below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd071d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this parameter to False if you don't want to locally deploy the ingestion NIMs\n",
    "deploy_rag_ingestion_locally = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aae4e3",
   "metadata": {},
   "source": [
    "Add the necessary environment variables so that the RAG deployment will use the localy deployment or the hosted NVIDIA AI Endpoint services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd06b03-eefd-47ba-af66-19f669765c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if deploy_rag_ingestion_locally:\n",
    "    # Set the endpoint urls of the ingestion NIMs to local\n",
    "    os.environ[\"APP_LLM_MODELNAME\"] = \"nvidia/llama-3.3-nemotron-super-49b-v1\"\n",
    "    os.environ[\"APP_LLM_SERVERURL\"] = \"\"\n",
    "    os.environ[\"APP_RANKING_SERVERURL\"] = \"\"\n",
    "    os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "    os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"nemoretriever-embedding-ms:8000\"\n",
    "    os.environ[\"EMBEDDING_NIM_ENDPOINT\"] = \"http://nemoretriever-embedding-ms:8000/v1\"\n",
    "    os.environ[\"PADDLE_INFER_PROTOCOL\"] = \"grpc\"\n",
    "    os.environ[\"PADDLE_GRPC_ENDPOINT\"] = \"paddle:8001\"\n",
    "    os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"grpc\"\n",
    "    os.environ[\"YOLOX_GRPC_ENDPOINT\"] = \"page-elements:8001\"\n",
    "    os.environ[\"YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT\"] = \"graphic-elements:8001\"\n",
    "    os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"grpc\"\n",
    "    os.environ[\"YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT\"] = \"table-structure:8001\"\n",
    "    os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"grpc\"\n",
    "    os.environ[\"ENABLE_RERANKER\"] = \"false\" #Disable re-ranking\n",
    "    os.environ[\"ENABLE_NV_INGEST_BATCH_MODE\"] = \"true\"\n",
    "    os.environ[\"USERID\"] = str(os.getuid())\n",
    "    # deploy the ingestion NIMs locally\n",
    "    # PLEASE NOTE this can take up to 15 minutes\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "                [\"docker\", \"compose\", \"-f\", \"rag/deploy/compose/nims.yaml\", \"--profile\", \"ingest\", \"up\", \"-d\"],\n",
    "                env=os.environ,\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "        )\n",
    "        print(result.stdout[-1000:], flush=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(e.stderr)\n",
    "else:\n",
    "    # If you do not want to deploy the RAG Ingestion NIMs locally, and want to use the public hosted endpoints instead:\n",
    "    os.environ[\"APP_LLM_MODELNAME\"] = \"nvidia/llama-3.3-nemotron-super-49b-v1\"\n",
    "    os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "    os.environ[\"APP_RANKING_MODELNAME\"] = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "    os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"\"\n",
    "    os.environ[\"APP_LLM_SERVERURL\"] = \"\"\n",
    "    os.environ[\"APP_RANKING_SERVERURL\"] = \"\"\n",
    "    os.environ[\"EMBEDDING_NIM_ENDPOINT\"] = \"https://integrate.api.nvidia.com/v1\"\n",
    "    os.environ[\"PADDLE_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\"\n",
    "    os.environ[\"PADDLE_INFER_PROTOCOL\"] = \"http\"\n",
    "    os.environ[\"YOLOX_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\"\n",
    "    os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "    os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    "    os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "    os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    "    os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\"\n",
    "    os.environ[\"ENABLE_RERANKER\"] = \"false\"\n",
    "    os.environ[\"ENABLE_NV_INGEST_BATCH_MODE\"] = \"true\"\n",
    "    os.environ[\"USERID\"] = str(os.getuid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829227d",
   "metadata": {},
   "source": [
    "Deploy the NVIDIA RAG blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f923aa-e17b-4c82-82cd-06d9ee65d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the vector db containers from the repo root. \n",
    "try:\n",
    "    result = subprocess.run(\n",
    "            [\"docker\", \"compose\", \"-f\", \"rag/deploy/compose/vectordb.yaml\", \"up\", \"-d\"],\n",
    "            env=os.environ,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3378a-adda-48d5-98b8-36eb2d3b5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the ingestion containers from the repo root. This pulls the prebuilt containers from NGC and deploys it on your system.\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "            [\"docker\", \"compose\", \"-f\", \"rag/deploy/compose/docker-compose-ingestor-server.yaml\", \"up\", \"-d\"],\n",
    "            env=os.environ,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d1d44-3a84-4982-aa5b-0c07ac8abd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the rag containers from the repo root. This pulls the prebuilt containers from NGC and deploys it on your system.\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "            [\"docker\", \"compose\", \"-f\", \"rag/deploy/compose/docker-compose-rag-server.yaml\", \"up\", \"-d\"],\n",
    "            env=os.environ,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49426d1a",
   "metadata": {},
   "source": [
    "Confirm all of the containers are running successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0d5a7-317e-49f9-98bf-a3b90c28b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm all the below mentioned containers are running.\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f88bc",
   "metadata": {},
   "source": [
    "The outputs should look like this if deploying the ingestion NIMs locally: \n",
    "\n",
    "| Container ID | Name | Status |\n",
    "|-------------|------|--------|\n",
    "| 4fc3494e0646 |  rag-playground                 |  Up 21 seconds |\n",
    "| 15177b220b11 |  rag-server                     |  Up 22 seconds |\n",
    "| 0941f2dc6039 |  compose-nv-ingest-ms-runtime-1 |  Up 28 seconds (healthy) |\n",
    "| a99de643c140 |  ingestor-server                |  Up 28 seconds |\n",
    "| 4b29311ad214 |  compose-redis-1                |  Up 28 seconds |\n",
    "| 7651a7c41bd0 |  milvus-standalone              |  Up 37 seconds |\n",
    "| 32056205632b |  milvus-minio                   |  Up 37 seconds (healthy) |\n",
    "| b1bc296b158a |  milvus-etcd                    |  Up 38 seconds (healthy) |\n",
    "| f4d9c05425a3 |  compose-page-elements-1        |  Up 7 minutes|\n",
    "| f83d24d482e9 |  compose-paddle-1               |  Up 7 minutes|\n",
    "| 8c911e965fca |  compose-table-structure-1      |  Up 7 minutes|\n",
    "| c1a5fac065a7 |  compose-graphic-elements-1     |  Up 7 minutes|\n",
    "| acdeb9c53261 |  nemoretriever-embedding-ms     |  Up 7 minutes (healthy)|\n",
    "\n",
    "Otherwise, the outputs should look like this:\n",
    "| Container ID | Name | Status |\n",
    "|-------------|------|--------|\n",
    "| 4fc3494e0646 |  rag-playground                 |  Up 21 seconds |\n",
    "| 15177b220b11 |  rag-server                     |  Up 22 seconds |\n",
    "| 0941f2dc6039 |  compose-nv-ingest-ms-runtime-1 |  Up 28 seconds (healthy) |\n",
    "| a99de643c140 |  ingestor-server                |  Up 28 seconds |\n",
    "| 4b29311ad214 |  compose-redis-1                |  Up 28 seconds |\n",
    "| 7651a7c41bd0 |  milvus-standalone              |  Up 37 seconds |\n",
    "| 32056205632b |  milvus-minio                   |  Up 37 seconds (healthy) |\n",
    "| b1bc296b158a |  milvus-etcd                    |  Up 38 seconds (healthy) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f5bd8",
   "metadata": {},
   "source": [
    "At this point, you should be able to access the NVIDIA RAG frontend web application by visiting `http://<your-server-ip>:8090`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219490a",
   "metadata": {},
   "source": [
    "<div class=\\\"alert alert-block alert-success\\\">\n",
    "    <b>Tip:</b> If you are running this notebook as a Brev Launchable or on Brev, you will need to make sure the port for the RAG playground is accessible. On the settings page for your machine from which you launched the notebook, navigate to the \"Access\" tab among three tabs \"Container Content Access\", scroll down to \"Using Ports\", if \"8090\" is not already listed, enter \"8090\", click \"Expose Port\", and then click \"I accept\". You should see the link in the format \"your-server-ip:8090\" now under the section \"Using Ports\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23ed3d",
   "metadata": {},
   "source": [
    "To test the RAG deployment:\n",
    "- Navigate to the RAG frontend web application exposed on port 8090.\n",
    "- On the left sidebar, click \"New Collection\".\n",
    "- Select a PDF to upload. We recommend starting with the file `notebooks/simple.pdf` included in the blueprint repository.\n",
    "- After the collection is created and the file is uploaded, select the collection by clicking on it in the left sidebar. \n",
    "- Ask a question in the chat like \"What is the title?\". Confirm that a response is given.\n",
    "\n",
    "*If any of these steps fail, please consult the NVIDIA RAG blueprint [troubleshooting guide](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/troubleshooting.md) and the [Biomedical AI-Q Research Agent troubleshooting guide](https://github.com/NVIDIA-AI-Blueprints/biomedical-aiq-research-agent/blob/main/docs/troubleshooting.md) prior to proceeding further*. For problems creating a collection or uploading a file, you can view the logs of the ingestor-server by running `docker logs ingestor-server`. For problems asking a question, you can view the logs of the rag-server by running `docker logs rag-server`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce666f3-2364-42ee-90b5-5bd506f94a3e",
   "metadata": {},
   "source": [
    "## Step 2: Deploy the Biomedical AI-Q Research Agent \n",
    "\n",
    "This NVIDIA blueprint allows you to create a Biomedical AI-Q Research Agent using NVIDIA NeMo Agent Toolkit, powered by NVIDIA NIM microservices.\n",
    "\n",
    "The research agent allows you to:\n",
    "- Provide a desired report structure and topic\n",
    "- Provide human in the loop feedback on a research plan\n",
    "- Perform parallel research of both unstructured on-premise data and web sources\n",
    "- Perform Virtual Screening when researching a condition or disease when discovering novel small-molecule therapies is intended\n",
    "- Update the draft report using Q&A \n",
    "- Q&A with the final report for further understanding\n",
    "- View sources from both RAG and web search\n",
    "\n",
    "The blueprint consists of a frontend web interface and a backend API service. To deploy Biomedical AI-Q Research Agent, follow the steps below in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852e531-95f8-43f3-925f-b6afc5371e67",
   "metadata": {},
   "source": [
    "1. Clone the Git repository biomedical-aiq-research-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99aa66b-c0ba-406a-aac8-8c7d84182972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/NVIDIA-AI-Blueprints/biomedical-aiq-research-agent.git\n",
    "%cd biomedical-aiq-research-agent/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef117450",
   "metadata": {},
   "source": [
    "2. Configure the Virtual Screening NIMs to be utilized in the Biomedical AI-Q Research Agent Blueprint\n",
    "\n",
    "You could choose to utilize the public NVIDIA AI Endpoints (option 1) for the BioNeMo NIMs needed for Virtual Screening, or deploy them locally (option 2). This notebook will default to the public NVIDIA AI Endpoints (option 1). For steps to locally deploy the BioNeMo NIMs (option 2), please see section `Deploy the BioNeMo NIMs for Virtual Screening in the Biomedical AI-Q Research Agent` in [docs/get-started/get-started-docker-compose.md](https://github.com/NVIDIA-AI-Blueprints/biomedical-aiq-research-agent/blob/main/docs/get-started/get-started-docker-compose.md#deploy-the-bionemo-nims-for-virtual-screening-in-the-biomedical-ai-q-research-agent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500db10e",
   "metadata": {},
   "source": [
    "Utilizing the public NVIDIA AI Endpoints for the BioNeMo NIMs requires a NVIDIA_API_KEY that has access to [MolMIM](https://build.nvidia.com/nvidia/molmim-generate) and [DiffDock](https://build.nvidia.com/mit/diffdock).\n",
    "\n",
    "We will also want to set the MolMIM and DiffDock URLs to the public endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ad0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your own NVIDIA API Key here\n",
    "NVIDIA_API_KEY = \"nvapi-your-api-key-here\"\n",
    "os.environ['NVIDIA_API_KEY'] = NVIDIA_API_KEY\n",
    "\n",
    "os.environ[\"MOLMIM_ENDPOINT_URL\"] = \"https://health.api.nvidia.com/v1/biology/nvidia/molmim/generate\" # public NVIDIA AI Endpoint \n",
    "os.environ[\"DIFFDOCK_ENDPOINT_URL\"] = \"https://health.api.nvidia.com/v1/biology/mit/diffdock\" # public NVIDIA AI Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3064059",
   "metadata": {},
   "source": [
    "3. Set the necessary environment variables for the service to use hosted NVIDIA NIM microservices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40408efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to true if you want to use the publicly hosted NIMs on the NVIDIA AI Endpoints for the LLM NIMs\n",
    "os.environ[\"AIRA_HOSTED_NIMS\"] = \"true\"\n",
    "\n",
    "# optional, if you want to use web search. Please visit https://www.tavily.com/ for API key and make sure you have enough credits.\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tavily-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57869c51",
   "metadata": {},
   "source": [
    "4. Deploy the Biomedical AI-Q Research Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495d3e2-2cf3-42f0-945b-08ebf18a287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To deploy the Biomedical AI-Q Research Agent run:\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "            [\"docker\", \"compose\", \"-f\", \"deploy/compose/docker-compose.yaml\", \n",
    "            \"--profile\", \"aira\", \"up\", \"-d\", \"--build\"],\n",
    "            env=os.environ,\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "        print(\"Failed to bring up aira profile: \" + e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc648402",
   "metadata": {},
   "source": [
    "Confirm the services have started successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b13ad",
   "metadata": {},
   "source": [
    "The list should include these three additional containers:\n",
    "```bash\n",
    "CONTAINER ID   NAMES                            STATUS\n",
    "a1fbeb65efad   aira-nginx                       Up 51 seconds\n",
    "75974fdadc1c   aira-backend                     Up 51 seconds\n",
    "abdab7c24989   aira-frontend                    Up 51 seconds\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0644d",
   "metadata": {},
   "source": [
    "You can access the Biomedical AI-Q Research Agent frontend web application at `http://<your-server-ip>:3001`. The backend API documentation at `http://<your-server-ip>:8051/docs`. **If any of the services failed to start, refer to the troubleshooting guide in the docs folder**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d86a7-1d6f-4b78-b773-08032251e81c",
   "metadata": {},
   "source": [
    "<div class=\\\"alert alert-block alert-success\\\">\n",
    "    <b>Tip:</b> If you are running this notebook as a Brev Launchable or on Brev, you will need to make sure the port for the Biomedical AI-Q Research Agent demo web frontend is accessible. On the settings page for your machine from which you launched the notebook, navigate to the \"Access\" tab among three tabs \"Container Content Access\", scroll down to \"Using Ports\", if \"3001\" is not already listed, enter \"3001\", click \"Expose Port\", and then click \"I accept\". You should see the link in the format \"your-server-ip:3001\" now under the section \"Using Ports\". To view the backend REST APIs, repeat these steps for port \"8051\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acbc2e",
   "metadata": {},
   "source": [
    "## Step 3: Upload Default Collections\n",
    "The demo web application includes two default report prompts. To support these prompts, the blueprint includes two example datasets. In this section we will upload the default datasets using a bulk upload helper. You can also upload your own files through the web interface.\n",
    "\n",
    "Start by running the Docker upload utility. **Note: this command can take upwards of 30 minutes to execute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"run\", \"-e\", \"RAG_INGEST_URL=http://ingestor-server:8082/v1\",\n",
    "        \"-e\", \"PYTHONUNBUFFERED=1\",\n",
    "        \"-v\", \"/tmp:/tmp-data\",\n",
    "        \"--network\", \"nvidia-rag\",\n",
    "        \"nvcr.io/nvidia/blueprint/aira-load-files:v1.0.0\"\n",
    "        ],\n",
    "        env=os.environ,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Failed to load datasets: \" + e.stderr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27090de3",
   "metadata": {},
   "source": [
    "At the end of the command, you should see a list of documents successfully uploaded for both the Financial_Dataset and the Biomedical_Dataset. You can also confirm the datasets were uploaded by visiting the web frontend and clicking on \"Collections\" in the left sidebar.\n",
    "\n",
    "If any of the file upload steps failed, consult the [NVIDIA RAG blueprint troubleshooting guide](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/troubleshooting.md) and the [Biomedical AI-Q Research Agent troubleshooting guide](https://github.com/NVIDIA-AI-Blueprints/biomedical-aiq-research-agent/blob/main/docs/troubleshooting.md) prior to proceeding further. You can check the logs of the ingestor-server by running `docker logs ingestor-server` and the ingestion process by running `docker logs compose-nv-ingest-ms-runtime-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c342b",
   "metadata": {},
   "source": [
    "## Step 4: Use the Biomedical AI-Q Research Agent\n",
    "\n",
    "Follow the instructions in the [demo walkthrough](https://github.com/NVIDIA-AI-Blueprints/biomedical-aiq-research-agent/blob/main/demo/README.md) to explore the Biomedical AI-Q Research Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3a9c3",
   "metadata": {},
   "source": [
    "## Step 5: Stop Services\n",
    "\n",
    "To stop all services, run the following commands:\n",
    "\n",
    "1. Stop the Biomedical AI-Q Research Agent services:\n",
    "```bash\n",
    "docker compose -f deploy/compose/docker-compose.yaml --profile aira down\n",
    "```\n",
    "\n",
    "2. Stop the RAG services:\n",
    "\n",
    "First navigate to the rag repository's root. Ensure you still have the variable `NGC_API_KEY` exported.\n",
    "\n",
    "```bash\n",
    "docker compose -f deploy/compose/docker-compose-rag-server.yaml down\n",
    "docker compose -f deploy/compose/docker-compose-ingestor-server.yaml down\n",
    "docker compose -f deploy/compose/vectordb.yaml down\n",
    "docker compose -f deploy/compose/nims.yaml down\n",
    "```\n",
    "3. Remove the cache directories:\n",
    "```bash\n",
    "sudo rm -rf (path-to-rag)/deploy/compose/volumes\n",
    "```\n",
    "\n",
    "To verify all services have been stopped, run:\n",
    "```bash\n",
    "docker ps\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47115cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
